---
title: "LiDAR processing with DBSCAN, PMF and CSF"
author: "kardemummabulle"
date: "2023-02-13"
fig_caption: yes
---

## SECTION I: set-up R environment and prepare data.
#### Batch install packages needed to run the script.
##### The function "all_packages" installs all packages specified in the parameter "packages" with their dependencies.
##### This is needed to ensure that the R environment has all packages needed to run the script.
```{r,error=FALSE,warning=FALSE,message=FALSE,eval=FALSE,echo=T}
all_packages <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[,"Package"])]
  if (length(new.pkg)) 
  install.packages(new.pkg,dependencies=TRUE)
  sapply(pkg,require,character.only=TRUE)
  }
packages <- c('lidR','dplyr','ggplot2','sf','sp','raster','rgdal','ggspatial','rgeos','fpc','dbscan','future','parallel','doParallel','rgl')
all_packages(packages)
```

#### Load installed packages into R environment.
##### Once all needed packages are installed, the script could be run from this line, when the packages are called into the R environment.
##### There is no need to install/re-install packages, once they are in place, they could be activated at any time. So the step above when packages are downloaded needs to be done only once.
```{r,error=FALSE,warning=FALSE,message=FALSE}
libs <- c('lidR','dplyr','ggplot2','sf','sp','raster','rgdal','ggspatial','rgeos','fpc','dbscan','future','parallel','doParallel','rgl')
lapply(libs,require,character.only=T)
```

#### Setup working directory and pathway for data import/export.
##### The data will be imported from this folder and the results will be saved there as well (unless specified otherwise).
```{r,error=FALSE,warning=FALSE,message=FALSE}
setwd("U:/data")
```

#### Import the shapefile with polygons covering study sites.
##### We import the shapefile and select only the polygon covering site 3.
```{r,error=FALSE,warning=FALSE,message=FALSE}
area <- st_read("U:/data/SHP/area.shp")
area_3 <- omrade %>% filter(Id %in% c("3"))
st_area(area_3) # check the area of the polygon (m2)
```

#### Create a grid.
##### Since the LAS-file is big and takes a lot of space on disk, it seems reasonable to split it into tiles for further processing. Create a 7 by 7 m grid rotated at 21.5 degrees to split the polygon in 286 tiles.
##### The rotation of the grid is optional. Here the purpose was to follow the spatial direction/layout of the study site to minimize the number of tiles upon tile splitting of the raw LAS-file.
##### This is needed to optimize script performance and running time. Note that the angle of grid rotation and the size of the grid is arbitrary.
```{r,error=FALSE,warning=FALSE,message=FALSE,out.width="50%",fig.retina=2,fig.align="center"}
rotang=21.5 # rotation angle
rot=function(a) matrix(c(cos(a),sin(a),-sin(a),cos(a)),2,2)
tran=function(geo,ang,center) (geo-center)*rot(ang*pi/180)+center

inpolygon <- area_3 %>% sf::st_geometry()
center <- st_centroid(st_union(inpolygon))

grid <- sf::st_make_grid(tran(inpolygon,-rotang,center),cellsize=7)
rotated_grid <- tran(grid,rotang,center)

# Plot the grid
plot(rotated_grid,col="grey")
```

```{r,error=FALSE,warning=FALSE,message=FALSE,eval=FALSE,echo=T}
# Add a unique ID for each tile and project them to SWEREF 99TM
rotated_grid <- rotated_grid %>% st_sf('geometry'= .,data.frame('tile_id'=1:length(.)))
st_crs(rotated_grid) <- 3006 

# Create a folder for export of grid tiles
ifelse(!dir.exists(file.path("U:/data/","GRID")),dir.create(file.path("U:/data/","GRID")),F)
ifelse(!dir.exists(file.path("U:/data/GRID","7x7")),dir.create(file.path("U:/data/GRID","7x7")),F)

# Splid the grid by tile ID and save as individual shapefiles
split_grid <- function(rotated_grid,...){
  ids <- as.character(unique(rotated_grid$tile_id))
  df.list <- list() 
  for(i in ids){
    dfname <- paste0("",i)
    df.list[[dfname]] <- rotated_grid[rotated_grid$tile_id==i,]
    setwd(paste("U:/data/GRID/7x7"))
    st_write(df.list[[i]],dsn=paste0("",i,".shp"))
    print(df.list)}
  }
split_grid(rotated_grid)
```

## SECTION II: DBSCAN on raw LAS-file.
#### Import the raw LAS-file. 
##### Only parameters x, y and z are being imported as these are the parameters of interest.
```{r,error=FALSE,warning=FALSE,message=FALSE}
(LASfile <- readLAS("U:/data/LAS_60m_SWEREF99TM_RH2000.las",select="xyz"))
```

#### Clip raw LAS-file to tiles.
##### This part will import all the shapefiles for tiles located in the GRID 7x7 folder and clip the raw LAS-file to the tiles' dimension.
##### This is needed to be able to process LAS tiles and run DBSCAN. DBSCAN is computationally heavy and will require less dense pointcloud to be able to run.
```{r,error=FALSE,warning=FALSE,message=FALSE,eval=FALSE,echo=T}
grid <- list.files('U:/data/GRID/7x7',pattern="\\.shp$",full.names=T,recursive=F,include.dirs=FALSE)
in_files <- extension(basename(grid),'.las')

# Create a folder for export of clipped LAS tiles
ifelse(!dir.exists(file.path("U:/data/","LAS_ID_7x7_raw")),dir.create(file.path("U:/data/","LAS_ID_7x7_raw")),F)
output_folder <- 'U:/data/LAS_ID_7x7_raw'   
out_files <- file.path(output_folder,in_files)

batch_clip <- function(grid){
  for (i in 1:length(grid)) {
    shp <- st_read(grid[i]) 
    las_tile <- clip_roi(LASfile,shp) 
    las_tile@header@PHB[["X scale factor"]] <- 0.01
    las_tile@header@PHB[["Y scale factor"]] <- 0.01
    las_tile@header@PHB[["Z scale factor"]] <- 0.01
    writeLAS(las_tile,paste(out_files[i]))}
  }
batch_clip(grid)
```

### DBSCAN 
##### More on DBSCAN here: https://cran.r-project.org/web/packages/dbscan/dbscan.pdf
##### The idea of using DBSCAN came from the Deng et al., 2022 as a suitable method for filtering ground points together with rocks/boulders while removing points associated with trees/bushes and other noise.
##### DBSCAN is a density-based clustering algorithm, introduced in Ester et al. 1996, which can be used to identify clusters of any shape in data set containing noise and outliers. DBSCAN stands for "Density-Based Spatial Clustering and Application with Noise". The basic idea behind density-based clustering approach is derived from a human intuitive clustering method: clusters are dense regions in the data space, separated by regions of lower density of points. In other words, the density of points in a cluster is considerably higher than the density of points outside the cluster (“areas of noise”). DBSCAN is based on this intuitive notion of “clusters” and “noise”. The key idea is that for each point of a cluster, the neighborhood of a given radius has to contain at least a minimum number of points.
##### The goal is to identify dense regions, which can be measured by the number of objects close to a given point. Two important parameters are required for DBSCAN: epsilon (“eps”) and minimum points (“minPts”). The parameter "eps" defines the radius of neighborhood around a point x. It’s called called the "epsilon-neighborhood" of x. The parameter "minPts" is the minimum number of neighbors within “eps” radius. Any point x in the dataset, with a neighbor count greater than or equal to "minPts", is marked as a core point; x is border point, if the number of its neighbors is less than "minPts", but it belongs to the "epsilon-neighborhood" of some core point z. Finally, if a point is neither a core nor a border point, then it is called a noise point or an outlier.
##### One should start by playing with the “minPts” that will form a cluster. “minPts” defines the minimum density around a core point (i.e., the minimum density for non-noise areas). Increasing the “minPts” suppresses more noise in the data and requires more points to form a cluster. Since the point cloud is normally very dense and the ground theoretically should be the most dense region, one could start at average density of the pointcloud. In this case the average density of the raw unclipped LAS-file is 1333.61 points/m², so 1200 can be a nice starting point. Note that the choice of the parameters is arbitrary and the different values will affect DBSCAN output.
##### Another key parameter is the “eps” value. A suitable “eps” given a fixed value for “minPts” can be found by inspecting the "kNNdistplot". The k-nearest neighbor distance plot sorts all data points by their k-nearest neighbor distance (here from cluster with 1200 points). So on x axis is the number of points sorted by distance and on y axis is the “eps” (or a proximity, how close they are to each other). A sudden increase of the kNN distance (a knee) indicates that the points to the right are most likely outliers. Choose “eps” for DBSCAN where the knee is. Note that the distance is not metric, it is Euclidean. In mathematics, the Euclidean distance between two points in Euclidean space is the length of a line segment between the two points. It can be calculated from the Cartesian coordinates of the points using the Pythagorean theorem, therefore is called the Pythagorean distance. Here the code is run with several “eps” but one "minPts" of 1200. The results are appended to LAS tiles and tiles are exported as separate LAS files.
```{r,error=FALSE,warning=FALSE,message=FALSE,out.width="50%",fig.retina=2,fig.align="center"}
# Let's check one of the tiles and try to pick DBSCAN parameters
(tile10 <- readLAS("U:/data/LAS_ID_7x7_raw/10.las",select="xyz"))
# This tile has a density at 2388.84 points/m²

# Convert to dataframe
tile10_df <- payload(tile10)

# Activate multiple CPUs (because the kNNdistplot function takes time to run and so does DBSCAN)
cluster <- makeCluster(detectCores()-1) # leave 1 core for OS
registerDoParallel(cluster)

# kNNdistplot with minPts of 1200 (Note that 1200 is arbitrary chosen)
dbscan::kNNdistplot(tile10_df,k=1200)
# Looks like the knee with minPts of 1200 is somewhere around 1.6. Despite that, we will run DBSCAN with the range of eps as different tiles have different densities and the variability inside the tiles is large in terms of numbers of trees, bushes, etc. So in reality each tile has a different minPts and eps values.
```

##### Run DBSCAN tile-wise.
```{r,error=FALSE,warning=FALSE,message=FALSE,message=FALSE,eval=FALSE,echo=T}
# Import all clipped raw LAS tiles and specify the output folder
grid <- list.files('U:/data/LAS_ID_7x7_raw',pattern="\\.las$",full.names=T,recursive=F,include.dirs=FALSE)
in_files <- extension(basename(grid),'.las')
output_folder <- ('U:/data/DBSCAN_7x7')   
out_files <- file.path(output_folder,in_files)

# Establish search parameters of minPts and range of different eps
k <- c(1200)
eps <- c(0.7,0.75,0.8,0.85,0.9,0.95,1,1.05,1.1,1.15,1.2,1.25,1.3,1.4,1.45,1.5,1.55,1.6,1.65,1.7,1.75,1.8,1.85,1.9,1.95,2)
grid_param <- expand.grid(k=k,eps=eps)

# Log the start time to find out the running time of the function
ptm <- proc.time()

# DBSCAN function
batch_dbscan <- function(grid){
  for (i in 1:length(grid)) {
    tile <- readLAS(grid[i],select="z") 
    tile_df <- payload(tile)
    if (nrow(tile_df)>1200){ 
      results <- mapply(grid_param$k,grid_param$eps,FUN=function(k,eps) 
      {cluster <- dbscan(tile_df,minPts=k,eps=eps)$cluster})  
      results_df <- as.data.frame(results)
      colnames(results_df) <- c("eps0.7","eps0.75","eps0.8","eps0.85","eps0.9","eps0.95","eps1","eps1.05","eps1.1","eps1.15","eps1.2","eps1.25","eps1.3",
                                "eps1.4","eps1.45","eps1.5","eps1.55","eps1.6","eps1.65","eps1.7","eps1.75","eps1.8","eps1.85","eps1.9","eps1.95","eps2")
      # Append results to LAS tiles
      tile <- add_lasattribute(tile,results_df[,1],"eps0.7","eps0.7")
      tile <- add_lasattribute(tile,results_df[,2],"eps0.75","eps0.75")
      tile <- add_lasattribute(tile,results_df[,3],"eps0.8","eps0.8")
      tile <- add_lasattribute(tile,results_df[,4],"eps0.85","eps0.85")
      tile <- add_lasattribute(tile,results_df[,5],"eps0.9","eps0.9")
      tile <- add_lasattribute(tile,results_df[,6],"eps0.95","eps0.95")
      tile <- add_lasattribute(tile,results_df[,7],"eps1","eps1")
      tile <- add_lasattribute(tile,results_df[,8],"eps1.05","eps1.05")
      tile <- add_lasattribute(tile,results_df[,9],"eps1.1","eps1.1")
      tile <- add_lasattribute(tile,results_df[,10],"eps1.15","eps1.15")
      tile <- add_lasattribute(tile,results_df[,11],"eps1.2","eps1.2")
      tile <- add_lasattribute(tile,results_df[,12],"eps1.25","eps1.25")
      tile <- add_lasattribute(tile,results_df[,13],"eps1.3","eps1.3")
      tile <- add_lasattribute(tile,results_df[,14],"eps1.4","eps1.4")
      tile <- add_lasattribute(tile,results_df[,15],"eps1.45","eps1.45")
      tile <- add_lasattribute(tile,results_df[,16],"eps1.5","eps1.5")
      tile <- add_lasattribute(tile,results_df[,17],"eps1.55","eps1.55")
      tile <- add_lasattribute(tile,results_df[,18],"eps1.6","eps1.6")
      tile <- add_lasattribute(tile,results_df[,19],"eps1.65","eps1.65")
      tile <- add_lasattribute(tile,results_df[,20],"eps1.7","eps1.7")
      tile <- add_lasattribute(tile,results_df[,21],"eps1.75","eps1.75")
      tile <- add_lasattribute(tile,results_df[,22],"eps1.8","eps1.8")
      tile <- add_lasattribute(tile,results_df[,23],"eps1.85","eps1.85")
      tile <- add_lasattribute(tile,results_df[,24],"eps1.9","eps1.9")
      tile <- add_lasattribute(tile,results_df[,25],"eps1.95","eps1.95")
      tile <- add_lasattribute(tile,results_df[,26],"eps2","eps2")
      # Scale for export
      tile@header@PHB[["X scale factor"]] <- 0.01
      tile@header@PHB[["Y scale factor"]] <- 0.01
      tile@header@PHB[["Z scale factor"]] <- 0.01
      writeLAS(tile,paste(out_files[i]))}
    # Clear memory
    rm(tile)
    rm(tile_df)
    rm(i)
    gc()
  }
}
batch_dbscan(grid)
proc.time()-ptm
# DBSCAN runs on 286 tiles in 178035.13 seconds or in 49.4542 hours, so it was a bit of overkill to run with 26 epsilons. Let's check if it was worth it.

# Stop cluster
on.exit(stopCluster(cluster))
```

#### Check the output.
##### Plot and look at the DBSCAN results for different tiles.
```{r,error=FALSE,warning=FALSE,message=FALSE,out.width="50%",fig.retina=2,fig.align="center"}
# Let's check one of the tiles
(tile110 <- readLAS("U:/data/DBSCAN_7x7/110.las"))

# Plot with eps value of 0.8 (one can change this parameter to see how plots for different eps values look like)
rgl::setupKnitr(autoprint=TRUE)
options(rgl.printRglwidget=FALSE)
options(rgl.useNULL=TRUE)
plot(tile110,color="eps0.8",pal=rainbow,legend=T)
widget=rglwidget()
print(widget)
# If one tries to plot with different eps values it will be visible that that for tile 110 some eps values fit better than others. Here 0.8 works best in my subjective opinion.
```

```{r,error=FALSE,warning=FALSE,message=FALSE,out.width="50%",fig.retina=2,fig.align="center"}
# What about tile 10
(tile10 <- readLAS("U:/data/DBSCAN_7x7/10.las"))

rgl::setupKnitr(autoprint=TRUE)
options(rgl.printRglwidget=FALSE)
options(rgl.useNULL=TRUE)
plot(tile10,color="eps0.95",pal=rainbow,legend=T) # here the eps is different
widget=rglwidget()
print(widget)
```

#### Summary: DBSCAN
##### Generally, DBSCAN succeeds at grouping points for trees and branches together, but the results vary for different tiles and their size. The tested tile sizes were: 3x3 m, 5x5, 7x7 m, 10x10 m, 15x15 m and 22.5x22.5 m (10x10 m, 15x15 m and 22.5x22.5 m were tested with pre-processed LAS-file (in TerraSolid), because DBSCAN won't run on such big dense tiles). The smaller tiles run faster and the larger ones take a long long time (as was just seen above). DBSCAN will run faster if the RAM memory on the computer is large and if there are several CPUs (here I had 13 cores). Also having less eps values would significantly speed up the running time. 
##### Individual tiles can be filtered based on the DBSCAN output and serve as an input for one more DBSCAN run (maybe with the second/third round one could succeed at removing unnecessary points and have a nicely filtered pointcloud). But his is both computationally/memory and time-demanding procedure that will require some fine-tuning and manual work. Also, in order to filter tiles properly one should create a more complex function that will be able to identify ground class that differ in each tile based on best-fit eps and respective densities that vary between the tiles. There is a lot of variability between the tiles and no tiles' results look comparable, so for example in one tile the best results will be with eps value of 0.8 and another tile might have eps value of 0.95, etc etc (as seen in the example above). This is not only due to the internal variability of each tile (what is inside the tile, whether it has trees, boulders, bushes, etc or only ground with minimal noise), but also due to how the script for DBSCAN is structered here. Ideally the DBSCAN should be trained on one single tile and then the output should be "predicted" for the rest of the data. But this is not possible because the LAS-file is too large to be "predicted to".


## SECTION III: Progressive Morphological Filter and Cloth Simulation Function.
#### Create a bigger grid.
##### Create a 22.5 by 22.5 m grid rotated at 21.5 degrees to split the polygon in 28 tiles.
```{r,error=FALSE,warning=FALSE,message=FALSE,out.width="50%",fig.retina=2,fig.align="center"}
rotang=21.5 # rotation angle
rot=function(a) matrix(c(cos(a),sin(a),-sin(a),cos(a)),2,2)
tran=function(geo,ang,center) (geo-center)*rot(ang*pi/180)+center

inpolygon <- area_3 %>% sf::st_geometry()
center <- st_centroid(st_union(inpolygon))

grid <- sf::st_make_grid(tran(inpolygon,-rotang,center),cellsize=22.5)
rotated_grid <- tran(grid,rotang,center)

# Plot the grid
plot(rotated_grid,col="grey")
```

```{r,error=FALSE,warning=FALSE,message=FALSE,eval=FALSE,echo=T}
# Add a unique ID for each tile and project them to SWEREF 99TM
rotated_grid <- rotated_grid %>% st_sf('geometry'= .,data.frame('tile_id'=1:length(.)))
st_crs(rotated_grid) <- 3006 

# Create a folder for export of grid tiles
ifelse(!dir.exists(file.path("U:/data/GRID","22.5x22.5")),dir.create(file.path("U:/data/GRID","22.5x22.5")),F)

# Splid the grid by tile ID and save as individual shapefiles
split_grid <- function(rotated_grid,...){
  ids <- as.character(unique(rotated_grid$tile_id))
  df.list <- list() 
  for(i in ids){
    dfname <- paste0("",i)
    df.list[[dfname]] <- rotated_grid[rotated_grid$tile_id==i,]
    setwd(paste("U:/data/GRID/22.5x22.5"))
    st_write(df.list[[i]],dsn=paste0("",i,".shp"))
    print(df.list)}
  }
split_grid(rotated_grid)
```

##### Clip raw LAS-file to tiles.
```{r,error=FALSE,warning=FALSE,message=FALSE,eval=FALSE,echo=T}
grid <- list.files('U:/data/GRID/22.5x22.5',pattern="\\.shp$",full.names=T,recursive=F,include.dirs=FALSE)
in_files <- extension(basename(grid),'.las')

# Create a folder for export of clipped LAS tiles
ifelse(!dir.exists(file.path("U:/data/","LAS_ID_22.5x22.5_raw")),dir.create(file.path("U:/data/","LAS_ID_22.5x22.5_raw")),F)
output_folder <- 'U:/data/LAS_ID_22.5x22.5_raw'   
out_files <- file.path(output_folder,in_files)

batch_clip <- function(grid){
  for (i in 1:length(grid)) {
    shp <- st_read(grid[i]) 
    las_tile <- clip_roi(LASfile,shp) 
    las_tile@header@PHB[["X scale factor"]] <- 0.01
    las_tile@header@PHB[["Y scale factor"]] <- 0.01
    las_tile@header@PHB[["Z scale factor"]] <- 0.01
    writeLAS(las_tile,paste(out_files[i]))}
  }
batch_clip(grid)
```

#### Progressive Morphological Filter (PMF). 
##### More on PMF here: https://r-lidar.github.io/lidRbook/gnd.html
##### PMF algorithm is based on the method described in Zhang et al., 2003.
##### Mathematical morphology composes operations based on set theory to extract features from an image. Two fundamental operations, dilation and erosion, are commonly employed to enlarge (dilate) or reduce (erode) the size of features in binary images. Dilation and erosion operations may be combined to produce opening and closing operations. The concept of erosion and dilation has been extended to multilevel (grayscale) images and corresponds to finding the minimum or maximum of the combinations of pixel values and the kernel function, respectively, within a specified neighborhood of each raster. These concepts can also be extended to the analysis of a continuous surface such as LIDAR data.
##### The combination of erosion and dilation generates opening and closing operations that are employed to filter LIDAR data. The opening operation is achieved by performing an erosion of the dataset followed by a dilation, while the closing operation is accomplished by carrying out a dilation first and then an erosion. An initial filtered surface is derived by applying an opening operation with a window of length l1 to the raw data. The large nonground features such as buildings are preserved because their sizes are larger than l1, while individual trees of size smaller than l1 are removed. For the terrain, features smaller than l1 are cut off and replaced by the minimum elevation within l1. In the next iteration, the window size is increased to l2, and another opening operation is applied to the filtered surface, resulting in a further smoothed surface. The building measurements are removed and replaced by the minimum elevation of previous filtered surface within l2, since the size of the building is smaller than the current window size. By performing an opening operation to LIDAR data with a line window that increases in size gradually, the progressive morphological filter can remove buildings and trees at various elevations.
##### PMF requires defining the following input parameters: "ws" (window size or sequence of window sizes in m), and "th" (elevation threshold or sequence of elevation thresholds in m).
##### The selection of the window size and elevation difference threshold is critical to achieve good results when applying the morphological filter. For window size selection, one straightforward choice is to increase the window size linearly by the following equation 4 from Zhang et al., 2003:
##### (4) wk=2kb+1
##### where k=1,2,.., and b is the initial window. Using equation 4 as the window size guarantees that the filter window is symmetric around the central point so that the programming of the opening operation is simplified and topographic features are being preserved. However, considerable computing time is needed for an area with large nonground objects.
##### The elevation threshold can be determined based on the slope of topography in the study area (equations 6-7 in Zhang et al., 2003). There is a relationship between the maximum elevation difference for the terrain, window size, and the terrain slope assuming that the slope is constant. But here in this script the sequence of window sizes and thresholds were arbitrary chosen and fine-tuned according to the output.

##### Import 22.5x22.5 m tiles into catalog for further processing.
```{r,error=FALSE,warning=FALSE,message=FALSE,out.width="50%",fig.retina=2,fig.align="center"}
# Create tile catalog
ctg <- readLAScatalog("U:/data/LAS_ID_22.5x22.5_raw")
plot(ctg)
```

##### Run PMF.
```{r,error=FALSE,warning=FALSE,message=FALSE,eval=FALSE,echo=T}
# Create output folder where the PMF LAS tiles will be saved with a suffix "_pmf"
ifelse(!dir.exists(file.path("U:/data/","PMF")),dir.create(file.path("U:/data/","PMF")),F)

outpath <- "U:/data/PMF"
opt_output_files(ctg) <- paste0(outpath,"/{*_pmf")

# Specify tile size and buffer. We leave tile size unchanged at 22.5x22.5 m but add 5 m buffer around each tile to minimize edge artifacts when running PMF
opt_chunk_size(ctg) <- 0
opt_chunk_buffer(ctg) <- 5

# Activate multiple CPUs
cluster <- makeCluster(detectCores()-1) # leave 1 core for OS
registerDoParallel(cluster)

# Run PMF
ws <- seq(0.5,1,1.5) # sequence of window sizes of 0.5, 1 and 1.5 m
th <- seq(0.05,0.1,length.out=length(ws)) # sequence of thresholds of 0.05 and 0.1 m

ptm <- proc.time()
pmf_ctg <- classify_ground(ctg,pmf(ws,th)) # result
proc.time()-ptm
# PMF runns in 568.98 seconds (9.483 minutes)

# Stop cluster
on.exit(stopCluster(cluster))
```

##### Merge PMF LAS tiles back into one single LAS file with PMF classification.
```{r,error=FALSE,warning=FALSE,message=FALSE,eval=FALSE,echo=T}
# Merge LAS tiles back into one PMF LAS file
ctg <- readLAScatalog("U:/data/PMF")
las <- readLAS(ctg)
opt_output_files(ctg) <- "U:/data/PMF/LAS_60m_SWEREF99TM_RH2000_pmf"
opt_chunk_buffer(ctg) <- 0
opt_chunk_size(ctg) <- 10000 # when merging tiles back it is umportant to specify tiles size bigger than the initial one. Here 1000 is an arbitrary number
singlefile_ctg <- catalog_retile(ctg)
```

##### Let's now look at the merged PMF LAS files.
```{r,error=FALSE,warning=FALSE,message=FALSE}
# We import the PMF LAS file with x,y,z and classification parameters
(las_pmf <- readLAS("U:/data/PMF/LAS_60m_SWEREF99TM_RH2000_pmf.las",select="xyzc"))

# Filter only ground class
(las_pmf_ground <- filter_poi(las_pmf,Classification==2))

# Export the filtered file
writeLAS(las_pmf_ground,file="U:/data/PMF/LAS_60m_SWEREF99TM_RH2000_pmf_ground.las")
```

##### Plot the result.
```{r,error=FALSE,warning=FALSE,message=FALSE,out.width="50%",fig.retina=2,fig.align="center"}
plot(las_pmf_ground,legend=T)
```

### Plot profiles (crossections).
#### This function can plot profiles for PMF LAS tiles.
#### Since the merged PMF LAS file is too big, there is no use at plotting it as the profiles won't be visible, but looking at the tiles' profiles might work.
```{r,error=FALSE,warning=FALSE,message=FALSE,out.width="50%",fig.retina=2,fig.align="center"}
# Import tile 10
(tile10_pmf <- readLAS("U:/data/PMF/10_pmf.las",select="xyzc"))

plot_crossection <- function(las,p1=c(min(las@data$X),mean(las@data$Y)),p2=c(max(las@data$X),mean(las@data$Y)),width=4,colour_by=NULL)
{colour_by <- enquo(colour_by)
  data_clip <- clip_transect(las,p1,p2,width)
  p <- ggplot(data_clip@data,aes(X,Z))+geom_point(size=0.5)+coord_equal()+theme_minimal()
  if (!is.null(colour_by))
  p <- p+aes(color=!!colour_by)+labs(color="")
  return(p)}

# Plot
plot_crossection(tile10_pmf,colour_by=factor(Classification))
```

#### Summary: PMF.
##### PMF is an easy-to-use filtering algorithm that can be fine-tuned and run both on tiles and on the original unclipped LAS file giving comparable outputs. The downside is that one should "guess" and fine-tune the window size and threshold parameters by him/herself. Ideally one should start playing with these parameters by pre-computing optinal values using equations from Zhang et al., 2003 paper. This solution is implemented in the "util_makeZhangParam" function (https://github.com/r-lidar/lidR/blob/master/R/algorithm-gnd.R) that can be fed into the pmf() function. But this option has not been tested.


#### Cloth Simulation Function (CSF). More on CSF here: https://r-lidar.github.io/lidRbook/gnd.html
##### CSF uses the Zhang et al., 2016 algorithm and consists of simulating a piece of cloth draped over a reversed point cloud. In this method the point cloud is turned upside down and then a cloth is dropped on the inverted surface. Ground points are determined by analyzing the interactions between the nodes of the cloth and the inverted surface. The cloth simulation itself is based on a grid that consists of particles with mass and interconnections that together determine the three-dimensional position and shape of the cloth.

##### CSF is a term of 3D computer graphics. It is also called cloth modeling, which is used for simulating cloth within a computer program. During cloth simulation, the cloth can be modeled as a grid that consists of particles with mass and interconnections, called a Mass-Spring Model. A particle on the node of the grid has no size but is assigned with a constant mass. The positions of the particles in three-dimensional space determine the position and shape of the cloth. In this model, the interconnection between particles is modeled as a “virtual spring”, which connects two particles and obeys Hooke’s law. To simulate the shape of the cloth at a speciﬁc time, the positions of all of the particles in the 3D space are computed: the position and velocity of a particle are determined by the forces that act upon it and can be quantified using Newton’s second law.
##### When applying the cloth simulation to LiDAR point ﬁltering, a number of modiﬁcations have been made to make this algorithm adaptable to point cloud ﬁltering. First, the movement of a particle is constrained to be in vertical direction, so the collision detection can be implemented by comparing the height values of the particle and the terrain (e.g., when the position of a particle is below or equal to the terrain, the particle intersects with the terrain). Second, when a particle reaches the “right position”, i.e., the ground, this particle is set as unmovable. Third, the forces are divided into two discrete steps to achieve simplicity and relatively high performance. Usually, the position of a particle is determined by the net force of the external and internal forces. In CSF the displacement of a particle from gravity is computed first (the particle is set as unmovable when it reaches the ground, so the collision force can be omitted) and then the position of this particle is modified according to the internal forces.
##### CSF algorithm needs several parameters: 
##### sloop_smooth - smoothness of the slope (when sharp slopes exist, set this parameter to TRUE to reduce errors).
##### class_threshold - the distance to the simulated cloth to classify point cloud into ground and non-ground. The default is 0.5.
##### cloth_resolution - the horizontal distance between paticles in cloth. This is usually set to the average distance of the points in the point cloud. The default value is 0.5.
##### rigidness - the rigidness of the cloth. 1 stands for soft cloth, 2 stands for medium cloth and 3 stands for hard one The default is 1. The larger the rigidness is, the more rigidly the cloth will behave.
##### iterations - maximum iteration for simulating cloth. The default value is 500. Usually, users do not need to change this.
##### time_step - controls the displacement of particles from gravity during each iteration. The default value is 0.65. Do not change this value. It is suitable for most cases.
```{r,error=FALSE,warning=FALSE,message=FALSE,out.width="50%",fig.retina=2,fig.align="center"}
# Create tile catalog
ctg <- readLAScatalog("U:/data/LAS_ID_22.5x22.5_raw")
plot(ctg)
```

##### Run CSF.
```{r,error=FALSE,warning=FALSE,message=FALSE,eval=FALSE,echo=T}
# Create output folder where the CSF LAS tiles will be saved with a suffix "_csf"
ifelse(!dir.exists(file.path("U:/data/","CSF")),dir.create(file.path("U:/data/","CSF")),F)

outpath <- "U:/data/CSF"
opt_output_files(ctg) <- paste0(outpath,"/{*_csf")

# Specify tile size and buffer. We leave tile size unchanged at 22.5x22.5 m but add 5 m buffer around each tile to minimize edge artifacts when running CSF
opt_chunk_size(ctg) <- 0
opt_chunk_buffer(ctg) <- 5

# Activate multiple CPUs
cluster <- makeCluster(detectCores()-1) # leave 1 core for OS
registerDoParallel(cluster)

# Run CSF
ptm <- proc.time()
csf_ctg <- classify_ground(ctg,algorithm=csf(sloop_smooth=F,class_threshold=0.03,cloth_resolution=0.03,rigidness=1,iterations=500,time_step=0.65)) # result
proc.time()-ptm
# CSF runns in 301.97 seconds (5.032833 minutes)

# Stop cluster
on.exit(stopCluster(cluster))
```

##### Merge CSF LAS tiles back into one single LAS file with CSF classification.
```{r,error=FALSE,warning=FALSE,message=FALSE,eval=FALSE,echo=T}
# Merge LAS tiles back into one CSF LAS file
ctg <- readLAScatalog("U:/data/CSF")
las <- readLAS(ctg)
opt_output_files(ctg) <- "U:/data/CSF/LAS_60m_SWEREF99TM_RH2000_csf"
opt_chunk_buffer(ctg) <- 0
opt_chunk_size(ctg) <- 10000 # when merging tiles back it is umportant to specify tiles size bigger than the initial one. Here 1000 is an arbitrary number
singlefile_ctg <- catalog_retile(ctg)
```

##### Let's now look at the merged CSF LAS files.
```{r,error=FALSE,warning=FALSE,message=FALSE}
# We import the CSF LAS file with x,y,z and classification parameters
(las_csf <- readLAS("U:/data/CSF/LAS_60m_SWEREF99TM_RH2000_csf.las",select="xyzc"))

# Filter only ground class
(las_csf_ground <- filter_poi(las_csf,Classification==2))

# Export the filtered file
writeLAS(las_csf_ground,file="U:/data/CSF/LAS_60m_SWEREF99TM_RH2000_csf_ground.las")
```

##### Plot the result.
```{r,error=FALSE,warning=FALSE,message=FALSE,out.width="50%",fig.retina=2,fig.align="center"}
plot(las_csf_ground,legend=T) 
```

##### Plot profiles (crossections).
```{r,error=FALSE,warning=FALSE,message=FALSE,out.width="50%",fig.retina=2,fig.align="center"}
# Import tile 10
(tile10_csf <- readLAS("U:/data/CSF/10_csf.las",select="xyzc"))

# Plot
plot_crossection(tile10_csf,colour_by=factor(Classification))
```

#### Summary: CSF.
##### CSF is also quite easy to run and fine-tune, but it can only be run on tiles and won't be able to process large LAS file. The function works well on both raw and pre-processed in TerraSolid LAS file and yields comparable outputs. There are alot of parameters one can play with.
#### End ʕ•ᴥ•ʔ
